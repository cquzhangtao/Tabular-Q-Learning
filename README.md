Neural networks are often used to approximate policies and value functions, but they do not guarantee convergence. Tabular-based algorithms can guarantee convergence and can be also used to solve problems having a relatively big state space as long as enough computation power supports them. If it is impossible to set up corresponding hardware, anyway, the state space can be reduced by some dimension reduction algorithms or domain knowledge first. For this reason, an agent with a tabular Q-learning algorithm is developed in this project and will be embedded into the simulation model to learn scheduling knowledge. The continuous features in state space are split into several levels by predefined min, max, and step size. 


