Neural networks are often used to approximate policies and value functions, but they do not guarantee convergence. Tabular-based algorithms can guarantee convergence and can be also used to solve problems having a relatively big state space as long as enough computation power supports them. If it is impossible to set up corresponding hardware, anyway, the state space can be reduced by some dimension reduction algorithms or domain knowledge first. For this reason, an agent with a Q-learning algorithm is embedded into the simulation model to learn through scheduling.
